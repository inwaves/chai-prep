You are provided with the same decision tree data structure as in the first question. Your task is to implement the class method DecisionTree.create to construct a decision tree from the input dataset xs and output ys according to the following procedure.

At each step, pick an attribute xs[i] that maximizes the information gain (see below). This attribute may take on d = xks[i] values, and so divides the data into subsets $E_1,\dots,E_d$. There are four possibilities for a subset $E_i$:

    If all examples are of the same class c, insert constant leaf node c.
    If set is empty, insert a constant leaf node with the plurality value in \cup_i E_i.
    Otherwise, the set contains more than one label.
        If there are attributes that have yet to be split on, recursively apply this algorithm on $E_i$.
        If no attributes left, examples are inconsistent. Insert a constant leaf node containing the plurality value in $E_i$.

The plurality value is the element occurring with the greatest frequency. If two elements occur with the same frequency, break ties by choosing the one with the smaller label.

The information gain is the expected reduction in entropy. The entropy of a discrete dataset D with N points taking on d possible values, and a count of $N_j$ points with value j is:
H(D) = -\sum_{j=1}^d \frac{N_j}{N} \log \left(\frac{N_j}{N}\right)
taking $0 \log 0$ to be 0. The information gain for attribute $X_i$ is defined as:
image.png
where k is the number of possible values for attribute $X_i$,
Y is the set of all class labels, N is the total number of data points, and $Y_{i,v} is the set of class labels for data points with image.png.

We provide you with a create_old function which follows a similar procedure to above, but:

    Assumes the number of classes, d, is always three.
    Uses a different heuristic to select attribute values, which results in different splitting decisions in the tree.
    Does not perform any input validation, whereas your implementation should verify that all attribute values in the input and class labels in the output fall within the specified range.

Other than these changes, we guarantee that create_old is implemented correctly, but do not necessarily endorse the design decisions made. We suggest that you modify and clean-up create_old to produce a correct implementation of create, however you are also free to write an implementation from scratch.

The test case Test.test_simple provides an illustrative example with the following simple dataset:

xs = [[0, 0], [2, 1], [0, 1], [2, 1], [1, 0], [0, 0], [1, 1]]
ys = [ 1,      2,      2,      2,      0,      1,      2    ]

Since the dataset contains many points with label 2, splitting on the second feature has very high information gain. This is because the right branch is entirely label 2 and so has zero entropy, while the left branch is much smaller in size than the original dataset and so has significantly lower entropy. This results in the same decision tree as the example for the first question:

inputs need to be in {0, 1, 2}, same for output